<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Technical Blogs on 蚊香鱼的Blog</title>
        <link>https://mosfish.github.io/wxyblog/categories/technical-blogs/</link>
        <description>Recent content in Technical Blogs on 蚊香鱼的Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Xiangyu Wen</copyright>
        <lastBuildDate>Fri, 16 May 2025 19:21:25 +0800</lastBuildDate><atom:link href="https://mosfish.github.io/wxyblog/categories/technical-blogs/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>BNNs入门学习</title>
        <link>https://mosfish.github.io/wxyblog/p/bnns%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/</link>
        <pubDate>Fri, 16 May 2025 19:21:25 +0800</pubDate>
        
        <guid>https://mosfish.github.io/wxyblog/p/bnns%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/</guid>
        <description>&lt;h1 id=&#34;bnns学习入门&#34;&gt;BNNs学习入门
&lt;/h1&gt;&lt;p&gt;贝叶斯神经网络(Bayesian Neural Networks)，相较于CNN之类的神经网络，输入单点数据、输出单点数据，贝叶斯神经网络可以输入一段概率分布，经过训练后可以生成一个输出的概率分布。所以采用贝叶斯的方式，模型训练后就可以变成——输入一个东西，而输出一个概率分布。在综述&lt;code&gt;Bayesian Neural Networks: An Introduction and Survey&lt;/code&gt;中提到，概率方法(BNN)提供的输出分布有助于开发可信模型，因其能识别预测中的不确定性，这当然也有一个范围哈。本学习笔记考虑了几个&lt;strong&gt;Literature Survey&lt;/strong&gt;和技术博客进行总结，非常感谢作者们的分享~&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;原文：Bayesian methods for performing regression with that of a simple neural network, and illustrates the importance of measuring uncertainty. While both methods perform well within the bounds of the training data, where extrapolation is required, the probabilistic method provides a full distribution of the function output as opposed to the point estimates provided by the NN. The distribution over outputs provided by probabilistic methods allows for the development of trustworthy models, in that they can identify uncertainty in a prediction. Given that NNs are the most promising model for generating AI systems, it is important that we can similarly trust their predictions.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;neural-networksnns&#34;&gt;Neural Networks(NNs)
&lt;/h2&gt;&lt;h3 id=&#34;基础架构&#34;&gt;基础架构
&lt;/h3&gt;&lt;p&gt;在讨论BNNs前还是先了解一下NN的结构。神经网络的基础架构——&lt;strong&gt;Multi-Layer Perceptron (MLP) network&lt;/strong&gt;，现代网络结构（如卷积网络）都具有等效的多层感知表示形式。简单的MLP搭建如下，输入$\phi$和输出$f$分别如下：
&lt;/p&gt;
$$
\phi_j = \sum_{i=1}^{N_1}a(x_iw_{ij}^1) \tag{1}\\
f_k = \sum_{j=1}^{N_2}g(\phi_jw_{jk}^2) 
$$&lt;p&gt;
示意图如下，中间为隐藏层，每个节点代表一个神经元或执行输入状态求和与激活的状态，箭头表示神经元间连接强度的参数（权重）：&lt;/p&gt;
&lt;div align=center&gt;&lt;img src=&#34;1.png&#34; width=300/&gt;&lt;/div&gt;
&lt;center&gt;&lt;font size=2&gt;用于二元分类或一维回归的具有单个隐藏层的神经网络架构示例&lt;/font&gt;&lt;/center&gt;
&lt;p&gt;其中Activation function（激活函数），类似于第一个公式，仿射变换+非线性变换$\phi(.)$，曾经是由$sign(.)$函数来表示，后来逐渐被Sigmoid, Hyperbolic Tangent (TanH), Rectified Linear Unit (ReLU) 和 Leaky-ReLU这几个函数取代。激活函数是很重要的一个概念，其图示如下，蓝色表示函数，红色表示函数的数值导数，&lt;/p&gt;
&lt;div align=center&gt;&lt;img src=&#34;2.png&#34; width=300/&gt;&lt;/div&gt;
&lt;center&gt;&lt;font size=2&gt;激活函数，(a) Sigmoid; (b) TanH; (c)ReLU; (d) Leaky-ReLU&lt;/font&gt;&lt;/center&gt;
&lt;h3 id=&#34;响应responding-derivatives&#34;&gt;响应(Responding derivatives)
&lt;/h3&gt;&lt;p&gt;使用Sigmoid函数时，原表达式等价于逻辑回归，那么网络的输出成为多个逻辑回归模型的总和。 对于回归模型函数$g(.)$将是恒等函数；二元分类则采用Sigmoid函数。其实公式(1)用矩阵表达更简便，前向传播过程(&lt;strong&gt;Forward propagation&lt;/strong&gt;)可以表示为
&lt;/p&gt;
$$
\mathbf{\Phi} = a(\mathbf{X^T W^1}) \tag{2}\\
\mathbf{F} = g(\mathbf{\Phi W^2})
$$&lt;h3 id=&#34;权重weight&#34;&gt;权重(Weight)
&lt;/h3&gt;&lt;p&gt;以&lt;strong&gt;反向传播&lt;/strong&gt;实现&lt;strong&gt;成本函数&lt;/strong&gt;最&lt;strong&gt;小&lt;/strong&gt;化：基于当前参数设置计算模型输出值，再求得各参数对应的偏导数，随后利用这些导数对每个参数进行迭代更新，其中$\alpha$表示学习率，迭代过程可以描述为：
&lt;/p&gt;
$$
w_{t+i} = w_t - \alpha\frac{ \partial J(x,y) }{ \partial w_t }. \tag{3}
$$&lt;h2 id=&#34;bayesian-neural-networks&#34;&gt;Bayesian Neural Networks
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;贝叶斯统计学派正是采用这种视角——将未知（或潜在）参数作为随机变量处理，并致力于通过训练数据中可观测的信息，学习这些参数的条件概率分布。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;这句翻译其实就是BNN的核心思想，考虑到BNN的学习过程中要通过已知数据来推测未知权重，这属于&lt;strong&gt;逆概率&lt;/strong&gt;问题，所以可以用贝叶斯定理解决，观测概率来表示这些&lt;strong&gt;权重&lt;/strong&gt; $\mathbf{\omega}$ 的分布，最终得到基于**观测数据 $D$ **的条件参数分布 $p(\omega, D)$ ，称之为后验分布(Posterior distribution)。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
