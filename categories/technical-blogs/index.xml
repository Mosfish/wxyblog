<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Technical Blogs on 蚊香鱼的Blog</title>
        <link>https://mosfish.github.io/wxyblog/categories/technical-blogs/</link>
        <description>Recent content in Technical Blogs on 蚊香鱼的Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Xiangyu Wen</copyright>
        <lastBuildDate>Fri, 16 May 2025 19:21:25 +0800</lastBuildDate><atom:link href="https://mosfish.github.io/wxyblog/categories/technical-blogs/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>BNNs入门学习</title>
        <link>https://mosfish.github.io/wxyblog/p/bnns%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/</link>
        <pubDate>Fri, 16 May 2025 19:21:25 +0800</pubDate>
        
        <guid>https://mosfish.github.io/wxyblog/p/bnns%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/</guid>
        <description>&lt;h1 id=&#34;bayesian-neural-networks&#34;&gt;Bayesian Neural Networks
&lt;/h1&gt;&lt;p&gt;贝叶斯神经网络(Bayesian Neural Networks)，相较于CNN之类的神经网络，输入单点数据、输出单点数据，贝叶斯神经网络可以输入一段概率分布，经过训练后可以生成一个输出的概率分布。所以采用贝叶斯的方式，模型训练后就可以变成——输入一个东西，而输出一个概率分布。在综述&lt;code&gt;Bayesian Neural Networks: An Introduction and Survey&lt;/code&gt;中提到，概率方法(BNN)提供的输出分布有助于开发可信模型，因其能识别预测中的不确定性，这当然也有一个范围哈。本学习笔记考虑了几个&lt;strong&gt;Literature Survey&lt;/strong&gt;和技术博客进行总结，非常感谢作者们的分享~&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;原文：Bayesian methods for performing regression with that of a simple neural network, and illustrates the importance of measuring uncertainty. While both methods perform well within the bounds of the training data, where extrapolation is required, the probabilistic method provides a full distribution of the function output as opposed to the point estimates provided by the NN. The distribution over outputs provided by probabilistic methods allows for the development of trustworthy models, in that they can identify uncertainty in a prediction. Given that NNs are the most promising model for generating AI systems, it is important that we can similarly trust their predictions.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;neural-networksnns&#34;&gt;Neural Networks(NNs)
&lt;/h2&gt;&lt;p&gt;在讨论BNNs前还是先了解一下NN的结构。神经网络的基础架构——&lt;strong&gt;Multi-Layer Perceptron (MLP) network&lt;/strong&gt;，现代网络结构（如卷积网络）都具有等效的多层感知表示形式。简单的MLP搭建如下，输入$\phi$和输出$f$分别如下：
&lt;/p&gt;
$$
\phi_j = \sum_{i=1}^{N_1}a(x_iw_{ij}^1) \tag{1}\\
f_k = \sum_{j=1}^{N_2}g(\phi_jw_{jk}^2) 
$$&lt;p&gt;
示意图如下，中间为隐藏层，每个节点代表一个神经元或执行输入状态求和与激活的状态，箭头表示神经元间连接强度的参数（权重）：&lt;/p&gt;
&lt;div align=center&gt;&lt;img src=&#34;1.png&#34; width=300/&gt;&lt;/div&gt;
&lt;center&gt;&lt;font face=&#34;黑体&#34; size=2&gt;用于二元分类或一维回归的具有单个隐藏层的神经网络架构示例&lt;/font&gt;&lt;/center&gt;
&lt;p&gt;其中Activation function（激活函数），类似于第一个公式，仿射变换+非线性变换$\phi(.)$，曾经是由$sign(.)$函数来表示，后来逐渐被Sigmoid, Hyperbolic Tangent (TanH), Rectified Linear Unit (ReLU) 和 Leaky-ReLU这几个函数取代。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
